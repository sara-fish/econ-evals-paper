{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "import os\n",
    "from econ_evals.utils.helper_functions import get_base_dir_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_subdirnames = os.listdir(get_base_dir_path() / \"experiments/scheduling/logs/\")\n",
    "log_subdirname_to_dirnames = {\n",
    "    log_subdirname: os.listdir(\n",
    "        get_base_dir_path() / \"experiments/scheduling/logs/\" / log_subdirname\n",
    "    )\n",
    "    for log_subdirname in log_subdirnames\n",
    "    if not log_subdirname.startswith(\".\")\n",
    "}\n",
    "\n",
    "# Read precomputed baseline data\n",
    "with open(\n",
    "    get_base_dir_path()\n",
    "    / \"experiments/scheduling/\"\n",
    "    / \"sample_scheduling_baseline_data.txt\",\n",
    "    \"r\",\n",
    ") as f:\n",
    "    difficulty_to_seed_to_baseline = literal_eval(f.read())\n",
    "\n",
    "print(\n",
    "    \"Overview of precomputed baseline data. (If missing a difficulty-seed pair you want to run, first run calculate_scheduling_baseline.py with the appropriate difficulties and seeds.)\"\n",
    ")\n",
    "for difficulty, seed_to_baseline in difficulty_to_seed_to_baseline.items():\n",
    "    seeds = sorted(list(seed_to_baseline.keys()))\n",
    "    print(f\"Difficulty {difficulty}: precomputed baseline for seeds {seeds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = []\n",
    "for log_subdirname in log_subdirnames:\n",
    "    # Skip hidden directories\n",
    "    if log_subdirname.startswith(\".\"):\n",
    "        continue\n",
    "    dirnames = log_subdirname_to_dirnames[log_subdirname]\n",
    "    short_log_subdirname = \"__\".join(log_subdirname.split(\"__\")[1:])\n",
    "    for dirname in dirnames:\n",
    "        if dirname.startswith(\".\"):\n",
    "            continue\n",
    "        df = pd.read_csv(\n",
    "            get_base_dir_path()\n",
    "            / \"experiments/scheduling/logs/\"\n",
    "            / f\"{log_subdirname}/{dirname}/logs.csv\"\n",
    "        )\n",
    "        global_params = pd.read_csv(\n",
    "            get_base_dir_path()\n",
    "            / \"experiments/scheduling/logs/\"\n",
    "            / f\"{log_subdirname}/{dirname}/global_params.csv\"\n",
    "        ).T.to_dict()[0]\n",
    "        # Calculate baseline, which is how well a naive solution would do (matching in order)\n",
    "\n",
    "        difficulty = short_log_subdirname.split(\"__\")[0]\n",
    "        seed = global_params[\"seed\"]\n",
    "        num_attempts = global_params[\"num_attempts\"]\n",
    "\n",
    "        final_attempt_df = df[df[\"attempt_num\"] == num_attempts - 1].reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "\n",
    "        final_num_blocking_pairs = len(\n",
    "            literal_eval(final_attempt_df[\"blocking_pairs\"].dropna().iloc[0])\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            baseline_data = difficulty_to_seed_to_baseline[difficulty][seed]\n",
    "        except KeyError:\n",
    "            print(\n",
    "                f\"Error: (difficulty {difficulty}, seed {seed}) not in baseline data. First, run the script calculate_scheduling_baseline.py, and make sure it is calculating baselines for all difficulty levels and seeds that you have data for.\"\n",
    "            )\n",
    "            continue\n",
    "        baseline_data = difficulty_to_seed_to_baseline[difficulty][seed]\n",
    "        baseline_num_blocking_pairs = baseline_data[\"baseline_num_blocking_pairs\"]\n",
    "        confidence_interval_low = baseline_data[\"confidence_interval_low\"]\n",
    "        confidence_interval_high = baseline_data[\"confidence_interval_high\"]\n",
    "\n",
    "        # final_num_blocking_pairs = int(df[\"num_blocking_pairs\"].dropna().iloc[-1])\n",
    "        final_score = 1 - final_num_blocking_pairs / baseline_num_blocking_pairs\n",
    "        final_score_floor = max(0, final_score)\n",
    "        table.append(\n",
    "            {\n",
    "                \"dirname\": dirname,\n",
    "                \"log_subdirname\": log_subdirname,\n",
    "                \"short_log_subdirname\": short_log_subdirname,\n",
    "                \"final_num_blocking_pairs\": final_num_blocking_pairs,\n",
    "                \"baseline_num_blocking_pairs\": baseline_num_blocking_pairs,\n",
    "                \"confidence_interval_low\": confidence_interval_low,\n",
    "                \"confidence_interval_high\": confidence_interval_high,\n",
    "                \"final_score\": final_score,\n",
    "                \"final_score_floor\": final_score_floor,\n",
    "                \"seed\": global_params[\"seed\"],\n",
    "                \"model\": global_params[\"model\"],\n",
    "                \"difficulty\": short_log_subdirname.split(\"__\")[0],\n",
    "            }\n",
    "        )\n",
    "df_table = pd.DataFrame(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate scheduling benchmark scores for each LLM and difficulty level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table[[\"model\", \"difficulty\", \"final_score\"]].groupby(\n",
    "    [\"model\", \"difficulty\"]\n",
    ").mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate full solve rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table[\"solved\"] = df_table[\"final_score\"] == 1\n",
    "df_table.groupby([\"model\", \"difficulty\"])[\"solved\"].sum() / df_table.groupby(\n",
    "    [\"model\", \"difficulty\"]\n",
    ")[\"solved\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "econ-evals-paper-BpVbBBNZ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
