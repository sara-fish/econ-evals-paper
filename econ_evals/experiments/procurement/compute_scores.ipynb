{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from econ_evals.utils.helper_functions import get_base_dir_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_subdirnames = os.listdir(get_base_dir_path() / \"experiments/procurement/logs/\")\n",
    "\n",
    "log_subdirname_to_dirnames = {\n",
    "    log_subdirname: os.listdir(\n",
    "        get_base_dir_path() / \"experiments/procurement/logs/\" / log_subdirname\n",
    "    )\n",
    "    for log_subdirname in log_subdirnames\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = []\n",
    "for log_subdirname in log_subdirnames:\n",
    "    dirnames = log_subdirname_to_dirnames[log_subdirname]\n",
    "    for dirname in dirnames:\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                get_base_dir_path()\n",
    "                / \"experiments/procurement/logs/\"\n",
    "                / f\"{log_subdirname}/{dirname}/logs.csv\"\n",
    "            )\n",
    "            global_params = pd.read_csv(\n",
    "                get_base_dir_path()\n",
    "                / \"experiments/procurement/logs/\"\n",
    "                / f\"{log_subdirname}/{dirname}/global_params.csv\"\n",
    "            ).to_dict(orient=\"records\")[0]\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "\n",
    "        model = global_params[\"model\"]\n",
    "\n",
    "        plot_df = df[\n",
    "            [\"attempt_num\", \"alloc\", \"cost\", \"utility\", \"is_feasible\"]\n",
    "        ].dropna()\n",
    "\n",
    "        # Calculate eval metrics\n",
    "        max_utility = plot_df[plot_df[\"is_feasible\"]][\"utility\"].max()\n",
    "\n",
    "        opt_utility = global_params[\"opt_utility\"]\n",
    "\n",
    "        max_ratio = max_utility / opt_utility\n",
    "\n",
    "        short_log_subdirname = \"__\".join(log_subdirname.split(\"__\")[1:])\n",
    "\n",
    "        table.append(\n",
    "            {\n",
    "                \"dirname\": dirname,\n",
    "                \"log_subdirname\": log_subdirname,\n",
    "                \"short_log_subdirname\": short_log_subdirname.split(model)[0] + model,\n",
    "                \"max_ratio\": max_ratio,\n",
    "                \"seed\": global_params[\"seed\"],\n",
    "                \"model\": model,\n",
    "                \"difficulty\": short_log_subdirname.split(\"__\")[0],\n",
    "                \"exploration_rate\": len(plot_df[\"alloc\"].unique()) / len(plot_df),\n",
    "            }\n",
    "        )\n",
    "\n",
    "df_table = pd.DataFrame(table)\n",
    "\n",
    "if len(df_table) == 0:\n",
    "    print(\n",
    "        \"Warning: no data in procurement/logs/. First collect benchmark data with run_procurement_batch.py.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate procurement benchmark scores for each LLM and difficulty level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table[[\"model\", \"difficulty\", \"max_ratio\"]].groupby(\n",
    "    [\"model\", \"difficulty\"]\n",
    ").mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table[[\"model\", \"difficulty\", \"exploration_rate\"]].groupby(\n",
    "    [\"model\", \"difficulty\"]\n",
    ").mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate full solve rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table[\"solved\"] = df_table[\"max_ratio\"] == 1\n",
    "df_table.groupby([\"model\", \"difficulty\"])[\"solved\"].sum() / df_table.groupby(\n",
    "    [\"model\", \"difficulty\"]\n",
    ")[\"solved\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
