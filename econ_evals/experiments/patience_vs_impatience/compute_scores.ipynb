{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from econ_evals.utils.helper_functions import get_base_dir_path\n",
    "import statistics\n",
    "\n",
    "from econ_evals.experiments.patience_vs_impatience.helper_functions import (\n",
    "    calc_future_value,\n",
    "    calc_reliability_score,\n",
    "    horizon_str_to_horizon_length,\n",
    "    detect_horizon_str,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dirnames = os.listdir(\n",
    "    get_base_dir_path() / \"experiments/patience_vs_impatience/logs\"\n",
    ")\n",
    "\n",
    "# Contains directory name and time horizon (in years) as key-value pairs\n",
    "dirname_to_horizon_length = {\n",
    "    get_base_dir_path()\n",
    "    / \"experiments/patience_vs_impatience/logs\"\n",
    "    / dirname: horizon_str_to_horizon_length(detect_horizon_str(dirname))\n",
    "    for dirname in log_dirnames\n",
    "}\n",
    "\n",
    "MODELS = [\"claude-3-5-sonnet-20241022\", \"gpt-4o-2024-11-20\", \"gemini-1.5-pro-002\"]\n",
    "\n",
    "num_trials = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate reliability and litmus scores for each time horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_rates = [1 + 0.001 * i for i in range(2000)]\n",
    "for dirname, timelength in dirname_to_horizon_length.items():\n",
    "    df = pd.read_csv(dirname / \"high_level_results.csv\")\n",
    "    # Sort data by 'value' so the line plot is in ascending order\n",
    "    df = df.sort_values(by=\"value\")\n",
    "    # Plot the number of times the model chose the delayed reward\n",
    "    df[\"future_reward\"] = num_trials - df[\"preferred_100_count\"]\n",
    "    # Normalize the counts by the number of trials\n",
    "    df[\"future_reward\"] = df[\"future_reward\"] / num_trials\n",
    "    for model in MODELS:\n",
    "        model_df = df[df[\"model\"] == model].copy()\n",
    "        reliability_scores = []\n",
    "        for interest_rate in interest_rates:\n",
    "            converted_interest_rate_value = calc_future_value(interest_rate, timelength)\n",
    "            reliability_scores.append(\n",
    "                calc_reliability_score(model_df, converted_interest_rate_value)\n",
    "            )\n",
    "        max_value = max(reliability_scores)\n",
    "        indices = [\n",
    "            i for i, value in enumerate(reliability_scores) if value == max_value\n",
    "        ]\n",
    "        calc_interest_rate = [interest_rates[i] for i in indices]\n",
    "        calc_interest_rate = statistics.median(calc_interest_rate)\n",
    "        interest_rate = calc_interest_rate * 100 - 100\n",
    "        print(\n",
    "            f\"Model: {model} with timeframe: {timelength:.3f}y, Max reliability score: {max_value:.3f}, Interest rates: {interest_rate:.2f}\"\n",
    "        )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate reliability and litmus scores aggregated over all time horizons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_rates = [1 + 0.001 * i for i in range(201)]\n",
    "data_records = []\n",
    "for model in MODELS:\n",
    "    reliability_scores = []\n",
    "    for interest_rate in interest_rates:\n",
    "        reliability_scores_for_specific_interest_rate = []\n",
    "        for dirname, timelength in dirname_to_horizon_length.items():\n",
    "            converted_interest_rate_value = calc_future_value(interest_rate, timelength)\n",
    "            df = pd.read_csv(dirname / \"high_level_results.csv\")\n",
    "            df = df.sort_values(by=\"value\")\n",
    "            df[\"future_reward\"] = num_trials - df[\"preferred_100_count\"]\n",
    "            df[\"future_reward\"] = df[\"future_reward\"] / num_trials\n",
    "            model_df = df[df[\"model\"] == model].copy()\n",
    "            reliability_scores_for_specific_interest_rate.append(\n",
    "                calc_reliability_score(model_df, converted_interest_rate_value)\n",
    "            )\n",
    "        # Aggregate the reliability scores for the different time lengths\n",
    "        reliability_score = sum(reliability_scores_for_specific_interest_rate) / len(\n",
    "            reliability_scores_for_specific_interest_rate\n",
    "        )\n",
    "        reliability_scores.append(reliability_score)\n",
    "        data_records.append(\n",
    "            {\n",
    "                \"model\": model,\n",
    "                \"interest_rate\": interest_rate,\n",
    "                \"reliability_score\": reliability_score,\n",
    "            }\n",
    "        )\n",
    "    max_value = max(reliability_scores)\n",
    "    indices = [i for i, value in enumerate(reliability_scores) if value == max_value]\n",
    "    calc_interest_rate = [interest_rates[i] for i in indices]\n",
    "    calc_interest_rate = statistics.median(calc_interest_rate)\n",
    "    interest_rate = calc_interest_rate * 100 - 100\n",
    "    print(f\"Model: {model}\")\n",
    "    print(f\"Interest rate: {interest_rate:.2f}\")\n",
    "    print(f\"Reliability scores: {max_value}\")\n",
    "    print()\n",
    "df = pd.DataFrame(data_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "econ-evals-paper-BpVbBBNZ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
